---
title: "assignment 3"
author: "Cheng"
date: "4/4/2019"
output:
  word_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, message = FALSE, error = FALSE)
library(tidyverse)
library(dplyr); library(tidyr); library(rstan); library(ggplot2); library(ggthemes);library(gridExtra)
theme_set(theme_tufte(base_family = 'sans'))

```


# Question1
_Fit two bivariate Gaussian regressions, using stan: (1) body weight as a
linear function of territory size (area), and (2) body weight as a linear function of
groupsize. Plot the results of these regressions, displaying the predicted regression line and
the 95% interval of the mean. Is either variable important for predicting fox body weight?_

```{r}
# load the data
data('foxes', package = 'rethinking')
d  <- foxes; rm(foxes)
# take a look
str(d)
```


## (1) model1 weight~area
```{r}
m3_1 <- "
  data {
    int<lower=0> N;
    vector[N] weight_;
    vector[N] area_;
  }
  parameters {
    real a;
    real bA;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + area_ * bA;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bA ~ normal(0, 10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  area_ = d$area
)

fitm3_1 <- stan(model_code = m3_1, data = dat, cores = 2, chains = 2, iter = 1000)
print(fitm3_1, probs = c(0.10, 0.5, 0.9))

```

## (2) model2 weight~groupsize
```{r}
m3_2 <- "
  data {
    int<lower=0> N;
    vector[N] weight_;
    vector[N] group_size;
  }
  parameters {
    real a;
    real bG;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + group_size * bG;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bG ~ normal(0, 10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  group_size = d$groupsize
)

fitm3_2 <- stan(model_code = m3_2, data = dat, cores = 2, chains = 2, iter = 1000)
print(fitm3_2, probs = c(0.10, 0.5, 0.9))

```

## (3) plot 2 models
```{r, fig.width = 6, fig.height = 3}
# plot model1
post <- as.data.frame(fitm3_1)
f_mu <- function(x) post$a + post$bA * x
area_new <- seq(1, 6)

mu <- 
  sapply(area_new, f_mu) %>%
  as_tibble() %>%
  rename_all(function(x) area_new) %>%
  mutate(Iter = row_number()) %>%
  gather(area_, weight_, -Iter) %>%
  group_by(area_) %>%
  mutate(pi_l = rethinking::PI(weight_, prob = 0.95)[1],
         pi_h = rethinking::PI(weight_, prob = 0.95)[2]) %>%
  mutate(mu = mean(weight_)) %>%
  ungroup() %>%
  mutate(area_ = as.numeric(area_))

p <- ggplot() 
p1 <- p + 
  geom_point(data = d,
             aes(area, weight), 
             shape = 1, color = 'dodgerblue') +
  geom_ribbon(data = mu,
              aes(x = area_, ymin = pi_l, ymax = pi_h), alpha = .1) +
  geom_line(data = mu,
            aes(x = area_, y = mu))

# plot model2 
post <- as.data.frame(fitm3_2)

f_mu <- function(x) post$a + post$bG * x
groupsize_new <- seq(2, 8)
mu <- 
  sapply(groupsize_new, f_mu) %>%
  as_tibble() %>%
  rename_all(function(x) groupsize_new) %>%
  mutate(Iter = row_number()) %>%
  gather(group_size, weight_, -Iter) %>%
  group_by(group_size) %>%
  mutate(pi_l = rethinking::PI(weight_, prob = 0.95)[1],
         pi_h = rethinking::PI(weight_, prob = 0.95)[2]) %>%
  mutate(mu = mean(weight_)) %>%
  ungroup() %>%
  mutate(group_size = as.numeric(group_size))

p2 <- p + 
  geom_point(data = d,
             aes(groupsize, weight), 
             shape = 1, color = 'dodgerblue') +
  geom_ribbon(data = mu,
              aes(x = group_size, ymin = pi_l, ymax = pi_h), alpha = .1) +
  geom_line(data = mu,
            aes(x = group_size, y = mu))

# plot together
grid.arrange(p1, p2, nrow = 1)
```

The coefficient of two regressions are 0.02 and -0.13, which means these two variables, both area and groupsize are not important to the outcome variable (weight). Besides, it appears that the relationship between area and weight is more weaker. 

# Question2
_Now fit a multiple linear regression with weight as the outcome and
both area and groupsize as predictor variables. Plot the predictions of the model for each
predictor, holding the other predictor constant at its mean. What does this model say about
the importance of each variable? Why do you get different results than you got in the
questions just above?_

## multiple linear regression model3
```{r}
m3_3 <- "
  data {
    int<lower=0> N;
    vector[N] weight_;
    vector[N] area_;
    vector[N] group_size;
  }
  parameters {
    real a;
    real bA;
    real bG;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + area_ * bA + group_size * bG;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bA ~ normal(0, 10);
    bG ~ normal(0, 10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  area_ = d$area,
  group_size = d$groupsize
)

fitm3_3 <- stan(model_code = m3_3, data = dat, cores = 2, chains = 2, iter = 1000)
print(fitm3_3, probs = c(0.10, 0.5, 0.9))
```

## create counterfatual plots of each variable
```{r}
post <- as.matrix(fitm3_3)

# setup the dataframe
nd1 <- 
  expand.grid(area_ = seq(1, 6), 
              group_size = seq(2, 8)) %>% 
  as.matrix

mu <- post[,1] + post[,2:3] %*% t(nd1)

avg <- colMeans(mu)
hdi <- apply(mu, 2, HDInterval::hdi)

# simulate the data of weight 
iter <- 1e4
y_hat <- matrix(nrow = iter, ncol = NROW(nd1))
for(i in 1:NROW(nd1)) y_hat[,i] <- rnorm(iter, post[,1] + post[,2:3] %*% as.matrix(nd1[i,]), post[,4])

y_hat_avg <- colMeans(y_hat)
y_hat_hdi <- apply(y_hat, 2, HDInterval::hdi)

nd1 <- 
  as_tibble(nd1) %>%
  bind_cols(avg = avg, 
            mu_hdi_l = hdi[1,], 
            mu_hdi_h = hdi[2,],
            y_hdi_l = y_hat_hdi[1,],
            y_hdi_h = y_hat_hdi[2,])

# plot for area
p1 <- ggplot(nd1, aes(x = area_, y = avg, group = group_size)) + 
  geom_line(data = nd1 %>% filter(group_size==5), color = 'black') +
  geom_ribbon(data = nd1 %>% filter(group_size==5),
              aes(x = area_, ymin = mu_hdi_l, ymax = mu_hdi_h), alpha = .1) +
  geom_ribbon(data =  nd1 %>% filter(group_size==5),
              aes(x = area_, ymin = y_hdi_l, ymax = y_hdi_h), alpha = .1) +
  labs(x = 'area', y = 'weight') 


# plot for groupsize
p2 <- ggplot(nd1, aes(x = group_size, y = avg, group = area_)) + 
  geom_line(data = nd1 %>% filter(area_==3), color = 'black') +
  geom_ribbon(data = nd1 %>% filter(area_==3),
              aes(x = group_size, ymin = mu_hdi_l, ymax = mu_hdi_h), alpha = .1) +
  geom_ribbon(data =  nd1 %>% filter(area_==3),
              aes(x = group_size, ymin = y_hdi_l, ymax = y_hdi_h), alpha = .1) +
  labs(x = 'groupsize', y = 'weight') 

grid.arrange(p1, p2, nrow = 1)
```

After running the multiple regression, we get the new coeffiecent of area and groupsize, which are 0.62 and -0.43. We can find that, compare to the bivariate model, we get the stronger relationlap that area is positively related to body weight and that group size is negatively related to body weight. Through the different outcome, we can say that there is the masking relationship between area and groupsize; however, it's hard to see this effect when we run the bivariate models.  

# Question3
_Finally consider the avgfood variable. Fit two more multiple regressions:
(1) body weight as an additive function of avgfood and groupsize, and (2) body weight as
an additive function of all three variables, avgfood and groupsize and area. Compare the
results of these models to the previous models youâ€™ve fit, in the first two questions._
  (a) _Is avgfood or area a better predictor of body weight? If you had to choose one or the
  other to include in a model, which would it be? Support your assessment with any
  tables or plots you choose._
  (b) _When both avgfood or area are in the same model, their effects are reduced (closer to
  zero) and their standard errors are larger than when they are included in separate
  models. Can you explain this results?_

## (1) create model4 weight~avgfood+groupsize
```{r}
m3_4 <- "
  data {
  int<lower=0> N;
  vector[N] weight_;
  vector[N] avg_food;
  vector[N] group_size;
  }
  parameters {
    real a;
    real bAV;
    real bG;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + avg_food * bAV + group_size * bG;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bAV ~ normal(0, 10);
    bG ~ normal(0, 10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  avg_food = d$avgfood,
  group_size = d$groupsize
)

fitm3_4 <- stan(model_code = m3_4, data = dat, cores = 2, chains = 2, iter = 1000)
print(fitm3_4, probs = c(0.10, 0.5, 0.9))

```

## (2) create model5 weight~avgfood+groupsize+area
```{r}
m3_5 <- "
  data {
    int<lower=0> N;
    vector[N] weight_;
    vector[N] avg_food;
    vector[N] group_size;
    vector[N] area_;
  }
  parameters {
    real a;
    real bAV;
    real bG;
    real bAR;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + avg_food * bAV + group_size * bG + area_ * bAR;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bAV ~ normal(0, 10);
    bG ~ normal(0, 10);
    bAR ~ normal(0,10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  avg_food = d$avgfood,
  group_size = d$groupsize,
  area_ = d$area
)

fitm3_5 <- stan(model_code = m3_5, data = dat, cores = 2, chains = 2, iter = 1000)
print(fitm3_5, probs = c(0.10, 0.5, 0.9))

```

```{r}
# standardize
d$avgfood.s <- (d$avgfood - mean(d$avgfood)) / sd(d$avgfood)
d$area.s <- (d$area - mean(d$area)) / sd(d$area)
```

## create bivariate model by area and avgfood to compare the standard deviation
```{r}
# model3.6 weight~avgfood.s+groupsize
m3_6 <- "
  data {
    int<lower=0> N;
    vector[N] weight_;
    vector[N] avg_food_s;
    vector[N] group_size;
  }
  parameters {
    real a;
    real bAV;
    real bG;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + avg_food_s * bAV + group_size * bG;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bAV ~ normal(0, 10);
    bG ~ normal(0, 10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  avg_food_s = d$avgfood.s,
  group_size = d$groupsize
)

fitm3_6 <- stan(model_code = m3_6, data = dat, cores = 2, chains = 2, iter = 1000)

# model3.7 weight~ area.s+groupsize
m3_7 <- "
  data {
    int<lower=0> N;
    vector[N] weight_;
    vector[N] area_s;
    vector[N] group_size;
  }
  parameters {
    real a;
    real bAV;
    real bG;
    real<lower=0, upper=10> sigma;
  }
  model {
    vector[N] mu = a + area_s * bAV + group_size * bG;
    weight_ ~ normal(mu, sigma);
    a ~ normal(10, 10);
    bAV ~ normal(0, 10);
    bG ~ normal(0, 10);
  }"

dat <- list(
  N = NROW(d),
  weight_ = d$weight,
  area_s = d$area.s,
  group_size = d$groupsize
)

fitm3_7 <- stan(model_code = m3_7, data = dat, cores = 2, chains = 2, iter = 1000)

print(fitm3_6)
print(fitm3_7)
```

(a) According to the result of moedel4, avgfood is positively related to weight and groupsize is negatively related to weight, which has the similiar result of the model3. Besides, consider the result of model5, it's easy to find that the coefficient of area and avgfood both decreased in magnitude which looks like there is the highly correlated relationship between area and avgfood. Therefore, I think there would be no problems to choose either avgfood or area but I would not put them together in the model since it would cause the multicolinearity problems.
To selcet a better predictor, I would like to run the bivariate models of each predictor and then compare the standard deviation to see which variable is more explanatory. We can see that the standard deviation of avgfood (0.26) is slightly bigger than area(0.19), so I would prefer avgfood as the better predictor in this case.

(b) It's because of the multicolinearity problems between area and avgfood which decrease the effect of each other when we put them in the same model.
